# Deep-Learning
Self-Learning Journey on Deep Learning. Based on the book <d2l.ai>

### Exercises Checklist
- 1. Introduction
  - [ ] Intro
- 2. Preliminaries
  - [ ] 2.1 Data Manipulation
  - [ ] 2.2 Data Preprocessing
  - [ ] 2.3 Linear Algebra
  - [x] 2.4 Calculus
  - [ ] 2.5 Automatic Differentiation
  - [ ] 2.6 Probability & Statistics
  - [ ] 2.7 Documentation
- 3. Linear Neural Networks for Regression
  - [ ] 3.1 Linear Regression
  - [x] 3.2 Object-Oriented Design for Implementation
  - [ ] 3.3 Synthetic Regression Data
  - [ ] 3.4 Linear Regression Implementation from Scratch
  - [ ] 3.5 Concise Implementation of Linear Regression
  - [ ] 3.7 Weight Decay
- 4. Linear Neural Networks for Classification
  - [ ] 4.2 MNIST
  - [ ] 4.3 Base Classification Model
  - [ ] 4.4 Softmax Regression
  - [ ] 4.5 Concise Implementation of Softmax Regression
- 5. Multilayer Perceptrons
  - [ ] 5.1 Multilayer Perceptrons
  - [ ] 5.2 Implementation of Multilayer Perceptrons
  - [ ] 5.4 Numerical Stability and Init
  - [ ] 5.6 Dropout
  - [ ] 5.7 Predicting House Prices on Kaggle
- 6. Builder's Guide
  - [ ] 6.1 Layers and Modules
  - [ ] 6.2 Parameters Management
  - [ ] 6.3 Parameters Initialization
  - [ ] 6.4 Lazy Initialization
  - [ ] 6.5 Custom Layers
  - [ ] 6.6 File I-O
  - [ ] 6.7 GPUs
- 7. Convolutional Neural Networks
  - [ ] 7.2 Convolutions for Images
  - [ ] 7.3 Padding and Stride
  - [ ] 7.4 Multiple Inputs & Output Channels
  - [ ] 7.6 LeNet
- 8. Modern Convolutional Neural Networks
  - [ ] 8.1 AlexNet
  - [ ] 8.2 VGG
  - [ ] 8.3 NiN
  - [ ] 8.4 GoogLeNet
  - [ ] 8.5 Batch Normalization
  - [ ] 8.6 Resnet & ResNeXt
  - [ ] 8.7 DenseNet
  - [ ] 8.8 Designing Convolution Network Architectures
- 9. Recurrent Neural Networks
  - [ ] 9.1 Working with Sequences
- 10. Modern Recurrent Neural Networks
- 11. Attention Mechanisms
  - [ ] 11.1 Queries, Keys, amd Values
  - [ ] 11.2 Attention Pooling by Similarity

- 22. Mathematics Appendix
  - [x] 22.7 Maximum Likelihood Estimation
