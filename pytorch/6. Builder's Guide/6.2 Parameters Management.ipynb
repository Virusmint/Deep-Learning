{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Davay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(8),\n",
    "                    nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Parameter Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.1524, -0.2269, -0.0838,  0.3527],\n",
       "                      [-0.3396,  0.4917, -0.0110,  0.2439],\n",
       "                      [-0.4935, -0.4033,  0.3169, -0.3528],\n",
       "                      [-0.3169,  0.2204, -0.0646,  0.0563],\n",
       "                      [ 0.4701,  0.2966, -0.4630, -0.3522],\n",
       "                      [-0.1054,  0.1039,  0.4977,  0.1254],\n",
       "                      [-0.1042, -0.3611, -0.4264, -0.1250],\n",
       "                      [-0.2957,  0.1996,  0.0184,  0.4052]])),\n",
       "             ('bias',\n",
       "              tensor([ 0.2786,  0.0591,  0.3286,  0.4545,  0.4938,  0.1245, -0.2854, -0.4378]))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.3355,  0.3066, -0.0034, -0.1305, -0.1553,  0.2115,  0.1366,  0.2557]])),\n",
       "             ('bias', tensor([-0.3024]))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targeted Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each parameter is represented as an instance of the parameter class. To do anything useful with the parameters, we first need to access the underlying numerical values. There are several ways to do this. Some are simpler while others are more general. The following code extracts the bias from the second neural network layer, which returns a parameter class instance, and further accesses that parameter’s value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.nn.parameter.Parameter, tensor([-0.3024]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net[2].bias), net[2].bias.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters are complex objects, containing values, gradients, and additional information. That is why we need to request the value explicitly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the value, each parameter also allows us to access the gradient. Because we have not invoked backpropagation for this network yet, it is in its initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing All Parameters at Once"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we need to perform operations on all parameters, accessing them one-by-one can grow tedious. The situation can grow especially unwieldy when we work with more complex modules (e.g., nested modules), since we would need to recurse through the entire tree to extract each sub-module’s parameters. \n",
    "$\\textbf{Below we demonstrate accessing the parameters of all layers.}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1524, -0.2269, -0.0838,  0.3527],\n",
       "          [-0.3396,  0.4917, -0.0110,  0.2439],\n",
       "          [-0.4935, -0.4033,  0.3169, -0.3528],\n",
       "          [-0.3169,  0.2204, -0.0646,  0.0563],\n",
       "          [ 0.4701,  0.2966, -0.4630, -0.3522],\n",
       "          [-0.1054,  0.1039,  0.4977,  0.1254],\n",
       "          [-0.1042, -0.3611, -0.4264, -0.1250],\n",
       "          [-0.2957,  0.1996,  0.0184,  0.4052]], requires_grad=True)),\n",
       " ('0.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.2786,  0.0591,  0.3286,  0.4545,  0.4938,  0.1245, -0.2854, -0.4378],\n",
       "         requires_grad=True)),\n",
       " ('2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.3355,  0.3066, -0.0034, -0.1305, -0.1553,  0.2115,  0.1366,  0.2557]],\n",
       "         requires_grad=True)),\n",
       " ('2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.3024], requires_grad=True))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[(name, param) for name, param in net.named_parameters()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tied Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we want to share parameters across multiple layers. Let’s see how to do this elegantly. In the following code we allocate a fully connected layer and then use its parameters specifically to set those of another layer. Here we need to run the forward propagation net(X) before accessing the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to give the shared layer a name so that we can refer to its\n",
    "# parameters\n",
    "shared = nn.LazyLinear(8)\n",
    "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "\n",
    "net(X)\n",
    "# Check whether the parameters are the same\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# Make sure that they are actually the same object rather than just having the\n",
    "# same value\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the NestMLP model defined in Section 6.1 and access the parameters of the various layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Random weight parameters that will not compute gradients and\n",
    "        # therefore keep constant during training\n",
    "        self.rand_weight = torch.rand((20, 20))\n",
    "        self.linear = nn.LazyLinear(20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(X @ self.rand_weight + 1)\n",
    "        # Reuse the fully connected layer. This is equivalent to sharing\n",
    "        # parameters with two fully connected layers\n",
    "        X = self.linear(X)\n",
    "        # Control flow\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0063, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),\n",
    "                                 nn.LazyLinear(32), nn.ReLU())\n",
    "        self.linear = nn.LazyLinear(16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('net.0.weight',\n",
       "              tensor([[-0.0877, -0.3585, -0.2953, -0.1443],\n",
       "                      [ 0.1216,  0.0731,  0.2099,  0.1792],\n",
       "                      [ 0.1633,  0.4825,  0.4716, -0.4584],\n",
       "                      [ 0.4360, -0.4739,  0.4699, -0.3693],\n",
       "                      [-0.4357,  0.1256, -0.3125,  0.1569],\n",
       "                      [-0.0351,  0.3847,  0.1077,  0.3922],\n",
       "                      [ 0.2950, -0.3949, -0.2951, -0.1670],\n",
       "                      [ 0.3111,  0.3471,  0.3015, -0.4196],\n",
       "                      [ 0.2460, -0.1181,  0.0334, -0.3424],\n",
       "                      [-0.4037, -0.3732,  0.4245, -0.0582],\n",
       "                      [ 0.1236,  0.0804, -0.2163, -0.0618],\n",
       "                      [ 0.0445,  0.0148,  0.3403, -0.3086],\n",
       "                      [ 0.2415,  0.4747,  0.2716, -0.2019],\n",
       "                      [ 0.3685, -0.3134, -0.3571,  0.1061],\n",
       "                      [ 0.4501,  0.3378, -0.0096, -0.4528],\n",
       "                      [-0.2607, -0.3124, -0.1625, -0.0790],\n",
       "                      [-0.2478,  0.0562, -0.4622,  0.3597],\n",
       "                      [ 0.1715,  0.0967,  0.3641, -0.0090],\n",
       "                      [ 0.2644,  0.4627,  0.4307, -0.4818],\n",
       "                      [-0.2296, -0.0479, -0.1814, -0.3784],\n",
       "                      [-0.4979,  0.2681,  0.4542, -0.4958],\n",
       "                      [-0.0989, -0.1522, -0.3258, -0.3132],\n",
       "                      [-0.2299,  0.2226,  0.0756,  0.1516],\n",
       "                      [ 0.3198,  0.0641,  0.2638, -0.3293],\n",
       "                      [ 0.4182, -0.3879,  0.3459,  0.2668],\n",
       "                      [-0.1760,  0.2460, -0.0444,  0.4684],\n",
       "                      [ 0.2965,  0.0712, -0.4949,  0.2028],\n",
       "                      [-0.1631,  0.3457,  0.0757, -0.0835],\n",
       "                      [-0.2772,  0.3369, -0.0450, -0.0441],\n",
       "                      [-0.2047,  0.2955,  0.3005, -0.1317],\n",
       "                      [ 0.2316,  0.4162,  0.4879,  0.0625],\n",
       "                      [ 0.3466,  0.4381,  0.0169, -0.4566],\n",
       "                      [-0.0173,  0.2753,  0.3073, -0.3081],\n",
       "                      [-0.2455, -0.0590,  0.2012,  0.2361],\n",
       "                      [-0.1086, -0.1007, -0.1233,  0.4428],\n",
       "                      [ 0.2874,  0.3622,  0.4273, -0.2372],\n",
       "                      [-0.4894, -0.3379, -0.3312, -0.3261],\n",
       "                      [ 0.4157, -0.0224, -0.2938,  0.3610],\n",
       "                      [ 0.2539, -0.4335, -0.2774, -0.2809],\n",
       "                      [-0.2544, -0.1426, -0.3263, -0.0399],\n",
       "                      [ 0.0910, -0.1717, -0.4261,  0.4219],\n",
       "                      [ 0.4074, -0.3310,  0.1235,  0.3518],\n",
       "                      [ 0.4675,  0.0687,  0.2449, -0.2417],\n",
       "                      [-0.0877,  0.0216, -0.3746,  0.2887],\n",
       "                      [-0.3865,  0.0327,  0.2827,  0.0392],\n",
       "                      [-0.2929, -0.0142,  0.1191, -0.2310],\n",
       "                      [-0.2118, -0.0550, -0.2831, -0.3611],\n",
       "                      [-0.1400,  0.1131, -0.2068, -0.0127],\n",
       "                      [ 0.2171, -0.3439, -0.1769,  0.2757],\n",
       "                      [ 0.2961, -0.2705, -0.0591,  0.3114],\n",
       "                      [-0.4617, -0.2675,  0.1872, -0.0007],\n",
       "                      [-0.1967, -0.2694,  0.4539,  0.4599],\n",
       "                      [ 0.0016,  0.1171, -0.2419,  0.0931],\n",
       "                      [ 0.4977, -0.4147, -0.1102,  0.2249],\n",
       "                      [-0.0767, -0.3228, -0.1254, -0.0985],\n",
       "                      [-0.0510, -0.0935,  0.4827,  0.4776],\n",
       "                      [ 0.0442,  0.3222,  0.1747, -0.4323],\n",
       "                      [ 0.0308, -0.2416,  0.0669, -0.2923],\n",
       "                      [ 0.3686,  0.3576, -0.4380, -0.2191],\n",
       "                      [-0.2891,  0.4284, -0.4869, -0.0296],\n",
       "                      [ 0.2349,  0.3733, -0.2686, -0.2295],\n",
       "                      [-0.2176, -0.2626,  0.4102,  0.4235],\n",
       "                      [-0.0233,  0.4195,  0.3502,  0.4091],\n",
       "                      [ 0.3144, -0.0460,  0.1725,  0.2577]])),\n",
       "             ('net.0.bias',\n",
       "              tensor([ 0.3279, -0.4890,  0.0447, -0.2426,  0.3591,  0.2185,  0.3966, -0.3301,\n",
       "                      -0.3223, -0.1995, -0.1702, -0.3652, -0.0519,  0.3901, -0.4564, -0.1093,\n",
       "                       0.1373, -0.0091,  0.1295, -0.3599,  0.1201, -0.1745, -0.4884,  0.1657,\n",
       "                      -0.1971, -0.0329, -0.4702, -0.2250, -0.3088, -0.3538,  0.1974, -0.3896,\n",
       "                       0.0663,  0.3817, -0.0534,  0.1435, -0.2207, -0.4374,  0.1563, -0.3335,\n",
       "                       0.2566,  0.0322, -0.0068, -0.1928,  0.1143,  0.1994,  0.1131, -0.0294,\n",
       "                      -0.2318,  0.4311,  0.2154,  0.0755, -0.0592, -0.4200, -0.0731,  0.0006,\n",
       "                      -0.3764, -0.4460, -0.2981, -0.2058,  0.4313, -0.2787,  0.2856, -0.0210])),\n",
       "             ('net.2.weight',\n",
       "              tensor([[-0.0018,  0.0088,  0.0512,  ...,  0.0584, -0.0998, -0.0553],\n",
       "                      [-0.0417,  0.0039,  0.0754,  ...,  0.0840,  0.0014, -0.0342],\n",
       "                      [ 0.0690, -0.0465, -0.0368,  ...,  0.0181,  0.0018,  0.1075],\n",
       "                      ...,\n",
       "                      [ 0.0951, -0.0997,  0.0437,  ..., -0.0909, -0.1144,  0.0258],\n",
       "                      [ 0.0706, -0.0218,  0.1170,  ...,  0.0316,  0.0109, -0.1181],\n",
       "                      [-0.0931,  0.0217,  0.0155,  ..., -0.0312,  0.1083, -0.0754]])),\n",
       "             ('net.2.bias',\n",
       "              tensor([ 0.0523,  0.0980,  0.0717, -0.0174, -0.0060,  0.0187,  0.0758,  0.0928,\n",
       "                      -0.0637,  0.1081,  0.0617, -0.0759, -0.0147, -0.0688, -0.0663, -0.0642,\n",
       "                       0.0322, -0.0651,  0.0576, -0.0224,  0.0864,  0.0041,  0.0050, -0.1170,\n",
       "                       0.0518,  0.1110, -0.0508,  0.0391,  0.1105, -0.0358,  0.0850,  0.0129])),\n",
       "             ('linear.weight',\n",
       "              tensor([[-1.1419e-02, -1.1200e-01,  1.3110e-01, -1.3407e-01, -5.5804e-02,\n",
       "                       -1.5740e-01, -9.5811e-02, -1.6162e-01,  8.4041e-02, -1.6967e-01,\n",
       "                        9.7642e-02,  1.4595e-01, -5.7445e-02, -9.3520e-02,  7.2294e-02,\n",
       "                        2.4150e-02, -1.0586e-01,  1.6020e-01,  8.3271e-02,  1.6743e-01,\n",
       "                       -1.5131e-01,  9.1395e-02,  7.4953e-02,  1.1890e-01, -4.6063e-02,\n",
       "                        9.4218e-02, -1.5541e-01,  4.4644e-02,  1.0499e-01, -1.4314e-01,\n",
       "                       -6.9568e-02,  1.7203e-01],\n",
       "                      [-1.6422e-01,  8.4542e-02,  7.0367e-02, -1.4593e-01, -1.0974e-01,\n",
       "                       -7.3526e-03,  1.6909e-01,  9.5829e-02, -6.9823e-02,  1.7358e-01,\n",
       "                        2.9080e-02,  8.5236e-02, -8.2642e-02,  1.1563e-01, -6.7589e-02,\n",
       "                        6.0063e-02,  1.2907e-01, -5.6133e-02,  1.5870e-01,  7.4589e-02,\n",
       "                        6.8167e-02,  1.1196e-01,  4.8437e-03,  2.3775e-03, -1.4968e-01,\n",
       "                        1.5557e-01,  8.0051e-02, -1.3059e-01, -1.0954e-01, -1.7902e-02,\n",
       "                       -3.0597e-02,  1.2214e-02],\n",
       "                      [ 1.4895e-01,  1.5903e-01,  8.1187e-02, -1.1338e-01,  1.2452e-01,\n",
       "                        1.3551e-01,  1.7657e-01, -8.5806e-02,  1.6956e-01, -1.1770e-01,\n",
       "                       -4.1475e-02,  9.0403e-02, -6.4835e-02,  9.1844e-02, -8.8404e-02,\n",
       "                       -1.2392e-01,  5.6390e-02,  3.8326e-02, -1.3652e-01, -1.1598e-01,\n",
       "                        1.2157e-01, -3.4251e-02, -1.1032e-01, -1.9926e-02,  1.6513e-01,\n",
       "                        6.2099e-02,  1.3590e-01,  7.7440e-02, -4.6975e-02,  1.2694e-01,\n",
       "                        1.0473e-01, -1.5306e-01],\n",
       "                      [ 1.0590e-01,  1.5030e-01, -8.2315e-02,  1.7185e-01, -5.9019e-02,\n",
       "                       -1.0947e-01,  6.0442e-02,  3.7231e-02, -3.3633e-02,  4.8048e-02,\n",
       "                       -4.7903e-02,  1.5917e-01,  4.3626e-02, -9.3916e-02, -5.1803e-02,\n",
       "                       -4.6567e-02,  1.6943e-01, -6.1572e-02, -1.1960e-01,  6.3097e-02,\n",
       "                        9.6406e-02, -1.5307e-01, -1.5724e-01, -7.0166e-02,  1.0822e-01,\n",
       "                        8.1918e-02, -1.4332e-01,  6.8864e-02,  1.2636e-01,  5.0871e-02,\n",
       "                        1.2726e-01, -5.6184e-03],\n",
       "                      [-1.3611e-01, -9.8630e-02,  1.4973e-01, -1.1251e-01,  1.4664e-01,\n",
       "                        7.7429e-02,  4.0503e-02, -1.3178e-01, -1.0441e-01,  7.2513e-03,\n",
       "                        1.4698e-01, -1.5922e-01,  1.1333e-01,  8.3131e-02, -8.5224e-02,\n",
       "                        1.3753e-01,  1.3733e-01, -9.4306e-02,  1.7537e-01, -1.7440e-01,\n",
       "                       -1.5269e-01,  7.1260e-03,  2.4259e-02, -9.3245e-02,  1.6911e-01,\n",
       "                       -9.7144e-02,  5.4585e-02,  1.0143e-01,  9.4164e-03, -4.2118e-02,\n",
       "                       -1.0977e-01,  1.3075e-01],\n",
       "                      [-9.8744e-02, -1.3311e-01,  1.1569e-01, -1.3973e-01, -9.3355e-02,\n",
       "                        1.4357e-01,  6.1037e-04, -1.3003e-01, -1.3337e-01, -3.7019e-02,\n",
       "                       -8.2590e-02, -1.1129e-02, -1.2366e-01,  1.6331e-01,  1.3366e-01,\n",
       "                        9.1850e-02, -1.6796e-01, -2.9369e-02, -8.3698e-02, -6.9123e-02,\n",
       "                       -1.1407e-02, -6.8055e-02,  4.3902e-02,  1.5313e-01, -1.1262e-01,\n",
       "                       -4.5818e-02,  1.4454e-02, -2.4865e-02,  7.1665e-02, -2.2809e-02,\n",
       "                       -1.3095e-01, -1.7935e-02],\n",
       "                      [-5.7872e-02,  2.0375e-02,  8.2138e-02,  1.2151e-01, -3.3206e-03,\n",
       "                        5.0921e-03,  7.0675e-03, -8.2129e-02, -1.6445e-01,  1.2288e-01,\n",
       "                        1.1111e-01,  2.3338e-02,  6.7325e-02, -1.2422e-01,  1.2908e-02,\n",
       "                        3.8732e-02,  1.1335e-01,  1.7279e-01,  7.4245e-02,  4.3028e-02,\n",
       "                       -4.2217e-02,  1.5240e-01,  1.1783e-01, -1.5417e-01, -4.7567e-02,\n",
       "                       -1.6719e-01, -1.2360e-01,  1.0230e-01,  1.0399e-01,  8.9563e-02,\n",
       "                       -9.8190e-02, -1.5103e-01],\n",
       "                      [-1.0429e-01,  2.5135e-02, -1.8431e-02,  5.5141e-02,  1.5646e-01,\n",
       "                        4.4980e-02, -8.1849e-02,  3.0138e-03, -8.6280e-02,  1.5831e-01,\n",
       "                       -6.0701e-02, -1.2745e-01, -1.2068e-01, -8.6599e-03, -1.0314e-01,\n",
       "                       -1.5259e-01,  1.3710e-02,  8.5930e-02,  1.0948e-01, -1.0394e-01,\n",
       "                       -1.6520e-01, -9.1505e-02, -3.0953e-02, -1.5746e-01,  1.5976e-01,\n",
       "                        1.2543e-01,  1.5684e-01,  2.1922e-02,  6.0481e-02,  1.4471e-01,\n",
       "                        1.5939e-01,  3.6787e-02],\n",
       "                      [-1.4334e-01,  5.8866e-02,  7.6215e-02, -6.6676e-02,  8.4325e-03,\n",
       "                       -1.1992e-01, -1.6541e-01,  1.5058e-02, -5.9866e-02,  1.0147e-01,\n",
       "                       -1.2432e-01,  1.3986e-01,  8.0998e-02, -1.0587e-01,  5.1972e-03,\n",
       "                        1.1065e-01, -1.4139e-01, -1.0091e-01,  6.7554e-02,  4.8731e-02,\n",
       "                       -6.5414e-02,  1.3297e-01, -1.5201e-01,  9.2580e-02, -8.7750e-02,\n",
       "                        1.5634e-01,  3.8053e-02,  1.2459e-01, -3.0589e-03,  8.9514e-02,\n",
       "                        1.5086e-01,  6.7574e-03],\n",
       "                      [-1.2336e-01,  1.3243e-01, -1.2336e-01,  6.0976e-02, -1.0105e-01,\n",
       "                       -1.5291e-01,  4.3724e-02, -1.2507e-01,  1.6126e-01, -8.1412e-02,\n",
       "                        1.8299e-05, -1.2214e-02,  2.6811e-02,  7.3729e-02,  7.8920e-02,\n",
       "                       -5.3657e-02, -1.3465e-02,  8.6411e-02,  7.6626e-02, -3.4447e-02,\n",
       "                       -1.1901e-01,  1.0759e-01,  1.6896e-01,  1.4390e-01,  1.4344e-01,\n",
       "                        1.4814e-01, -1.1772e-01, -1.3834e-02,  9.2097e-02,  5.7350e-02,\n",
       "                       -7.9274e-02, -1.6184e-01],\n",
       "                      [ 1.2452e-01,  1.7613e-01,  4.6954e-02, -1.1359e-01,  4.0958e-02,\n",
       "                       -1.1047e-01,  1.4394e-01,  1.3956e-01, -1.6788e-01, -1.5755e-01,\n",
       "                        1.1595e-01, -1.0067e-01,  1.3054e-01,  1.1393e-01,  1.2086e-01,\n",
       "                       -8.1171e-02,  7.4811e-02,  9.7149e-02, -1.9783e-02, -4.9534e-02,\n",
       "                        1.6220e-01, -4.1442e-02, -1.0657e-01, -8.9130e-02, -1.5484e-02,\n",
       "                        6.1766e-02, -1.4205e-01, -9.3873e-02, -2.0760e-02,  1.5497e-01,\n",
       "                       -8.4293e-02, -5.4805e-02],\n",
       "                      [ 8.9273e-02,  6.9860e-02, -4.1180e-02,  8.1714e-02,  4.5698e-02,\n",
       "                        1.5124e-01,  9.1934e-02,  6.2118e-02,  3.7078e-02, -1.5089e-01,\n",
       "                       -9.6625e-02, -9.0945e-02,  1.5906e-01, -7.9559e-04, -1.3161e-01,\n",
       "                        2.8579e-02,  1.5638e-01,  2.4395e-02, -1.8113e-02,  6.2671e-02,\n",
       "                       -1.4892e-01,  1.1555e-01,  3.8525e-02, -8.2809e-02, -9.6678e-02,\n",
       "                        9.3828e-02, -1.3740e-01,  2.7484e-02,  4.7257e-02, -8.7260e-02,\n",
       "                        8.8706e-02,  6.6985e-03],\n",
       "                      [ 2.6149e-02, -3.7146e-03, -6.9293e-02,  3.0884e-02, -7.3697e-02,\n",
       "                       -1.5105e-01, -1.2862e-01,  4.7900e-02,  8.6876e-02,  5.1301e-02,\n",
       "                       -2.3150e-02,  8.1376e-02, -7.8723e-02, -1.5400e-01, -1.2197e-01,\n",
       "                       -1.2970e-01, -1.3515e-01,  1.1766e-01,  7.6904e-02,  1.6559e-01,\n",
       "                        1.3232e-01,  6.1856e-02,  7.6131e-02,  1.4827e-01,  9.7181e-03,\n",
       "                        1.1638e-01,  1.4615e-01, -2.7981e-02,  4.9036e-02, -1.1212e-01,\n",
       "                       -6.1465e-02,  3.0084e-02],\n",
       "                      [ 5.1636e-02, -2.2136e-02,  2.8475e-02,  8.1877e-02,  2.4629e-02,\n",
       "                        2.8211e-02,  8.6403e-02,  5.4872e-02,  1.6714e-01, -1.7606e-01,\n",
       "                       -5.9387e-02,  1.6181e-01, -3.4785e-02,  1.5767e-01,  1.1424e-01,\n",
       "                       -3.2026e-02, -9.9781e-02,  1.1289e-01, -1.1939e-01, -1.5640e-01,\n",
       "                        8.2027e-02,  1.6741e-01,  2.5674e-03,  8.1465e-05, -5.8165e-02,\n",
       "                       -8.5822e-02, -6.7393e-02, -1.1951e-01, -6.9571e-02, -1.3448e-02,\n",
       "                        2.9179e-02,  3.1663e-02],\n",
       "                      [ 7.3419e-02,  8.4700e-03, -1.1224e-01, -1.5302e-02,  1.7112e-01,\n",
       "                        1.1789e-01,  1.0290e-01,  6.2022e-02,  8.4831e-02,  5.4259e-03,\n",
       "                       -9.5992e-02, -1.5764e-01, -6.4425e-02, -3.6696e-02, -9.9879e-02,\n",
       "                        1.0975e-01, -4.9580e-02,  1.0668e-01, -4.7928e-03, -6.6421e-02,\n",
       "                        1.4081e-01, -8.0642e-02, -1.6727e-01, -9.7608e-03,  1.9124e-04,\n",
       "                        3.6239e-02,  1.2224e-01, -1.3933e-01,  9.2666e-02,  4.0715e-02,\n",
       "                       -9.4367e-02, -1.5244e-01],\n",
       "                      [-4.0316e-02, -1.7459e-01,  1.3716e-01,  7.4771e-02, -3.4624e-02,\n",
       "                        2.9936e-02,  1.3351e-01,  6.7703e-03, -1.5085e-01, -1.7096e-01,\n",
       "                        1.1247e-01,  6.7712e-02, -1.7573e-01,  1.3042e-01,  1.1225e-01,\n",
       "                        1.5889e-01,  5.3475e-02,  1.3780e-01,  3.8024e-03, -4.4192e-03,\n",
       "                        1.2136e-01, -3.2299e-02,  1.6027e-01,  5.0004e-02, -1.5002e-01,\n",
       "                       -3.1616e-02, -6.7552e-02, -1.1034e-01, -1.4214e-01,  2.1671e-02,\n",
       "                       -6.4805e-03,  1.5992e-01]])),\n",
       "             ('linear.bias',\n",
       "              tensor([ 0.0597, -0.0794, -0.0114, -0.1538,  0.1715,  0.1586, -0.0380,  0.0422,\n",
       "                       0.0343, -0.0191,  0.0088, -0.0005,  0.0748,  0.0047, -0.1753,  0.0724]))])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chimera[0].state_dict() # LazyLinear(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[-0.0042,  0.0870,  0.1771, -0.1444, -0.0761, -0.1452, -0.0505,  0.0137,\n",
       "                        0.0983,  0.1258, -0.1773,  0.0215, -0.1587,  0.2031, -0.1518, -0.2098,\n",
       "                        0.1247, -0.1080, -0.1618, -0.0983],\n",
       "                      [-0.0761,  0.1452,  0.0302,  0.0623, -0.0286, -0.0878, -0.2207,  0.0261,\n",
       "                        0.0422,  0.1852,  0.0320, -0.1799, -0.1590, -0.1376, -0.1658,  0.0840,\n",
       "                        0.1087,  0.0202, -0.0120, -0.0416],\n",
       "                      [ 0.0781, -0.0271,  0.1569,  0.2100, -0.1961, -0.1364,  0.0422,  0.1499,\n",
       "                        0.1536,  0.2049,  0.0187, -0.0110,  0.2135, -0.0289,  0.1336,  0.2051,\n",
       "                       -0.1148, -0.1143, -0.1050, -0.1406],\n",
       "                      [-0.1714, -0.0527,  0.0056, -0.1244,  0.0910,  0.1127,  0.0014, -0.2120,\n",
       "                       -0.1094,  0.1068,  0.0126,  0.1019,  0.2132, -0.0732,  0.0567, -0.0507,\n",
       "                       -0.1826,  0.0602,  0.1707,  0.2125],\n",
       "                      [ 0.1044, -0.1220, -0.0099, -0.1279, -0.0397,  0.0622, -0.1870,  0.0556,\n",
       "                        0.1896,  0.0142, -0.2132, -0.0744, -0.0829,  0.1057,  0.1055, -0.1242,\n",
       "                       -0.0718,  0.1119,  0.1241,  0.0128],\n",
       "                      [ 0.0830,  0.1636, -0.0507, -0.0963, -0.2003, -0.1130, -0.1348,  0.1823,\n",
       "                        0.1663,  0.1677,  0.0980, -0.0013, -0.0824, -0.1942, -0.1454, -0.2078,\n",
       "                       -0.2118, -0.0282,  0.0920,  0.1689],\n",
       "                      [-0.1764, -0.0290,  0.2076,  0.1603,  0.1488, -0.1947, -0.1915, -0.0164,\n",
       "                        0.1869, -0.1105, -0.0938,  0.1846,  0.0762,  0.0898,  0.0249,  0.1884,\n",
       "                        0.0953, -0.1204,  0.0419, -0.1486],\n",
       "                      [-0.1427, -0.0148, -0.1699, -0.1425,  0.0844,  0.1644,  0.0899, -0.1549,\n",
       "                        0.0782,  0.2037, -0.0184,  0.0285, -0.2140, -0.1112,  0.1315,  0.1913,\n",
       "                       -0.0263, -0.1068,  0.1419,  0.0721],\n",
       "                      [-0.0414, -0.0278, -0.1507, -0.0448, -0.1859,  0.1650, -0.0876, -0.0491,\n",
       "                        0.0939,  0.0589, -0.2118,  0.0914,  0.0790,  0.0299,  0.2194,  0.1811,\n",
       "                        0.0328, -0.1138,  0.1917,  0.0389],\n",
       "                      [-0.0006, -0.0130,  0.0579,  0.1986,  0.1672,  0.1973,  0.0454,  0.2178,\n",
       "                       -0.1891, -0.0168,  0.0971,  0.0413,  0.0091,  0.1202,  0.1026, -0.0484,\n",
       "                       -0.1936, -0.0922, -0.0248, -0.0497],\n",
       "                      [ 0.1799, -0.0595,  0.1627, -0.1980,  0.0227,  0.2041, -0.0085,  0.1373,\n",
       "                        0.0197,  0.0902,  0.2231,  0.1482, -0.0278, -0.1009,  0.2213, -0.2203,\n",
       "                       -0.0071,  0.0574,  0.0230, -0.0135],\n",
       "                      [ 0.0664, -0.1844,  0.0774, -0.0321,  0.1926,  0.0013, -0.0883, -0.1046,\n",
       "                        0.1696, -0.1624, -0.2081, -0.0674, -0.1290, -0.0275,  0.1030,  0.1121,\n",
       "                        0.1154,  0.0342,  0.1567,  0.1078],\n",
       "                      [-0.0459,  0.1636,  0.0549,  0.1478, -0.1671,  0.0162, -0.0847,  0.1374,\n",
       "                       -0.1863,  0.1182,  0.2161, -0.0347, -0.1433, -0.1676, -0.1383,  0.0596,\n",
       "                       -0.0653,  0.1043, -0.1752, -0.1329],\n",
       "                      [-0.1318, -0.1554, -0.0664,  0.0015,  0.1502,  0.1733, -0.1542,  0.0617,\n",
       "                        0.1754,  0.1585,  0.0797,  0.1130,  0.0080, -0.1521,  0.0176, -0.1248,\n",
       "                       -0.0935, -0.0510,  0.1867, -0.1493],\n",
       "                      [-0.0362,  0.0593, -0.1707, -0.1948,  0.1339, -0.1092,  0.1783, -0.1085,\n",
       "                        0.1445, -0.0438,  0.1815, -0.0199, -0.1073, -0.2072, -0.0036, -0.1150,\n",
       "                       -0.0207, -0.0183,  0.2220, -0.1550],\n",
       "                      [ 0.0576,  0.1663, -0.0467,  0.1783,  0.0853,  0.1798, -0.2225,  0.1239,\n",
       "                       -0.1399,  0.0555, -0.0479,  0.1232,  0.1114, -0.0488,  0.0997,  0.0852,\n",
       "                       -0.1879, -0.0549, -0.1472, -0.0482],\n",
       "                      [-0.1960, -0.1574,  0.2114,  0.1690, -0.1910,  0.1672, -0.2136, -0.1861,\n",
       "                       -0.1652,  0.0188,  0.0896,  0.2134, -0.0806,  0.1961,  0.1463, -0.1984,\n",
       "                       -0.1838, -0.0351, -0.0507, -0.0807],\n",
       "                      [ 0.0582, -0.1941,  0.2055, -0.0335, -0.2074, -0.1134,  0.2236, -0.0356,\n",
       "                       -0.1232,  0.1399, -0.1205, -0.1812,  0.0194, -0.0994,  0.0251,  0.1033,\n",
       "                        0.0550,  0.1599, -0.1566,  0.0934],\n",
       "                      [ 0.0994, -0.1403,  0.1651,  0.0902,  0.2174,  0.0505,  0.1700, -0.1951,\n",
       "                       -0.0871, -0.1108, -0.0510,  0.0884, -0.1450, -0.0647, -0.0316, -0.1058,\n",
       "                       -0.0087,  0.0702,  0.1258, -0.0190],\n",
       "                      [ 0.2038,  0.0901, -0.0555, -0.0724, -0.1685, -0.0822, -0.1317, -0.1065,\n",
       "                        0.0713,  0.0136,  0.0986, -0.0416,  0.1848, -0.0247, -0.1871, -0.0018,\n",
       "                        0.0096, -0.1310,  0.0626, -0.1144]])),\n",
       "             ('linear.bias',\n",
       "              tensor([ 0.0776, -0.1239,  0.0413,  0.0589,  0.1447, -0.0120, -0.0450,  0.0169,\n",
       "                       0.0484,  0.1185,  0.1610, -0.1477,  0.0527,  0.2068,  0.0654,  0.0068,\n",
       "                      -0.0700,  0.1286, -0.1650,  0.0540]))])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chimera[2].state_dict() # LazyLinear(32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Construct an MLP containing a shared parameter layer and train it. During the training process, observe the model parameters and gradients of each layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Why is sharing parameters a good idea?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
