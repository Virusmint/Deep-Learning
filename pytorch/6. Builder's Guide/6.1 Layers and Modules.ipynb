{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Davay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Modules\n",
    "\n",
    "Perhaps the easiest way to develop intuition about how a module works is to implement one ourselves. Before we implement our own custom module, we briefly summarize the basic functionality that each module must provide:\n",
    "\n",
    "1. Ingest input data as arguments to its forward propagation method.\n",
    "\n",
    "2. Generate an output by having the forward propagation method return a value. Note that the output may have a different shape from the input. For example, the first fully connected layer in our model above ingests an input of arbitrary dimension but returns an output of dimension 256.\n",
    "\n",
    "3. Calculate the gradient of its output with respect to its input, which can be accessed via its backpropagation method. Typically this happens automatically.\n",
    "\n",
    "4. Store and provide access to those parameters necessary to execute the forward propagation computation.\n",
    "\n",
    "5. Initialize model parameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Davay\\Desktop\\D2L\\pytorch\\6. Builder's Guide\\6.1 Layers and Modules.ipynb Cell 4\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Davay/Desktop/D2L/pytorch/6.%20Builder%27s%20Guide/6.1%20Layers%20and%20Modules.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMLP\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Davay/Desktop/D2L/pytorch/6.%20Builder%27s%20Guide/6.1%20Layers%20and%20Modules.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Davay/Desktop/D2L/pytorch/6.%20Builder%27s%20Guide/6.1%20Layers%20and%20Modules.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39m# Call the constructor of the parent class nn.Module to perform\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Davay/Desktop/D2L/pytorch/6.%20Builder%27s%20Guide/6.1%20Layers%20and%20Modules.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39m# the necessary initialization\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Davay/Desktop/D2L/pytorch/6.%20Builder%27s%20Guide/6.1%20Layers%20and%20Modules.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call the constructor of the parent class nn.Module to perform\n",
    "        # the necessary initialization\n",
    "        super().__init__()\n",
    "        self.hidden = nn.LazyLinear(256)\n",
    "        self.out = nn.LazyLinear(10)\n",
    "\n",
    "    # Define the forward propagation of the model, that is, how to return the\n",
    "    # required model output based on the input X\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Davay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Module._call_impl of MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (out): Linear(in_features=256, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self.add_module(str(idx), module)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for module in self.children():\n",
    "            X = module(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1122,  0.0859, -0.2232,  ..., -0.0600, -0.0186,  0.1708],\n",
      "        [ 0.0847, -0.2044,  0.0703,  ...,  0.0160,  0.1959, -0.0731],\n",
      "        [ 0.0519, -0.0074, -0.2062,  ...,  0.1979,  0.1533, -0.1174],\n",
      "        ...,\n",
      "        [ 0.0142,  0.1063,  0.1867,  ...,  0.2196,  0.2173,  0.0346],\n",
      "        [-0.0436,  0.0210,  0.0175,  ...,  0.1043,  0.1294, -0.0509],\n",
      "        [-0.2066,  0.0915,  0.1587,  ...,  0.0182,  0.1433,  0.1089]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0744, -0.0752, -0.0932,  0.0568, -0.0046,  0.1442, -0.0900,  0.1181,\n",
      "        -0.0853,  0.0547, -0.1647, -0.0904,  0.1134,  0.1859, -0.1241,  0.1729,\n",
      "        -0.0707,  0.1233,  0.1122, -0.1999,  0.0577, -0.0870,  0.0948, -0.0215,\n",
      "         0.0444, -0.2002,  0.1599, -0.1995, -0.2103,  0.0156, -0.0781, -0.1529,\n",
      "         0.0795,  0.1441,  0.1189, -0.1591, -0.1069, -0.1543,  0.0878,  0.0084,\n",
      "        -0.0670,  0.2094, -0.2191,  0.0397, -0.1138, -0.0639,  0.0694,  0.2042,\n",
      "         0.0322, -0.0591, -0.1231, -0.0588, -0.0565, -0.1237,  0.0812, -0.1557,\n",
      "         0.0995,  0.1037, -0.1907, -0.0024,  0.0962,  0.1728, -0.0077, -0.2150,\n",
      "        -0.1669, -0.1903,  0.0193, -0.1128, -0.0453,  0.0146, -0.1128, -0.1265,\n",
      "         0.0129, -0.0480,  0.1038, -0.1774,  0.1105, -0.0658,  0.1471,  0.1136,\n",
      "        -0.0297, -0.1706,  0.0096,  0.0850,  0.1163,  0.0386,  0.0241,  0.1817,\n",
      "        -0.1574,  0.1146,  0.1101, -0.1983, -0.1172, -0.1373, -0.1782, -0.2032,\n",
      "         0.0279, -0.1558,  0.1056, -0.1643, -0.0865, -0.0116,  0.1979, -0.0947,\n",
      "        -0.0920, -0.0890, -0.1785, -0.1813, -0.2135, -0.1389, -0.1252,  0.2043,\n",
      "        -0.1627,  0.0104, -0.1105,  0.1852,  0.0833, -0.0513,  0.1955, -0.0487,\n",
      "        -0.0075,  0.0334,  0.2221, -0.0098, -0.1091,  0.1573,  0.0755,  0.1147,\n",
      "         0.0203, -0.1586,  0.1491,  0.0819, -0.2145, -0.1251,  0.2026, -0.1987,\n",
      "        -0.1277,  0.0611,  0.1853,  0.1354,  0.0255, -0.1894, -0.1260,  0.1811,\n",
      "         0.1992, -0.0797,  0.1827, -0.2031,  0.0058, -0.1648,  0.0274, -0.0334,\n",
      "         0.1736,  0.1914,  0.0716, -0.1076,  0.1343,  0.2149,  0.0283, -0.1396,\n",
      "         0.0057, -0.1627,  0.0504,  0.1957,  0.0413, -0.0942,  0.1851, -0.1220,\n",
      "        -0.0571, -0.1994, -0.0071, -0.2096,  0.2127,  0.0172,  0.2034, -0.0096,\n",
      "        -0.0091,  0.0890,  0.0204, -0.0390,  0.1444, -0.1886,  0.1453,  0.1636,\n",
      "         0.2139, -0.1573,  0.0529, -0.1673, -0.1077, -0.0669,  0.0655,  0.1499,\n",
      "        -0.0101, -0.0439,  0.1465,  0.0929, -0.1561, -0.1419, -0.2221, -0.0095,\n",
      "         0.0580, -0.0677,  0.2010, -0.1385, -0.0025,  0.1516,  0.0029, -0.1286,\n",
      "         0.1964, -0.0633,  0.1448,  0.2016, -0.2023,  0.1097, -0.1413,  0.1172,\n",
      "        -0.0356,  0.1764, -0.2065, -0.1398,  0.0849,  0.1844,  0.0876, -0.0686,\n",
      "         0.1210, -0.0525,  0.0098, -0.0650, -0.1260,  0.0176, -0.0525, -0.1022,\n",
      "        -0.1983,  0.1670, -0.2053, -0.1775,  0.0995, -0.0882, -0.2184,  0.1415,\n",
      "        -0.0021,  0.1967,  0.1648,  0.0559,  0.1933,  0.1315,  0.0934,  0.0028,\n",
      "        -0.1194,  0.1118,  0.1188, -0.1310, -0.1712,  0.0503, -0.0099, -0.1446],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0018,  0.0549,  0.0377,  ..., -0.0087, -0.0620, -0.0344],\n",
      "        [-0.0105, -0.0506, -0.0549,  ...,  0.0396,  0.0510,  0.0056],\n",
      "        [ 0.0483,  0.0609, -0.0387,  ..., -0.0269, -0.0054,  0.0595],\n",
      "        ...,\n",
      "        [ 0.0359,  0.0455, -0.0609,  ..., -0.0042, -0.0549,  0.0282],\n",
      "        [-0.0552,  0.0425, -0.0393,  ...,  0.0289, -0.0004, -0.0341],\n",
      "        [-0.0236, -0.0313,  0.0234,  ..., -0.0256, -0.0545,  0.0157]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0057, -0.0555,  0.0251, -0.0154,  0.0139,  0.0073,  0.0004, -0.0553,\n",
      "        -0.0541,  0.0078], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "net = MySequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "net(X)\n",
    "for param in net.parameters():\n",
    "    print(param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Random weight parameters that will not compute gradients and\n",
    "        # therefore keep constant during training\n",
    "        self.rand_weight = torch.rand((20, 20))\n",
    "        self.linear = nn.LazyLinear(20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(X @ self.rand_weight + 1)\n",
    "        # Reuse the fully connected layer. This is equivalent to sharing\n",
    "        # parameters with two fully connected layers\n",
    "        X = self.linear(X)\n",
    "        # Control flow\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2227, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can mix and match various ways of assembling modules together. In the following example, we nest modules in some creative ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1063, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),\n",
    "                                 nn.LazyLinear(32), nn.ReLU())\n",
    "        self.linear = nn.LazyLinear(16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What kinds of problems will occur if you change MySequential to store modules in a Python list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequentialEx(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.net = [module for module in args]\n",
    "\n",
    "    def forward(self, X):\n",
    "        for module in self.net:\n",
    "            X = module(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1412, -0.0165,  0.0156, -0.0573,  0.0665, -0.0294, -0.2216,  0.0310,\n",
       "          0.1866,  0.0849],\n",
       "        [-0.2358, -0.1653,  0.0419, -0.0169,  0.0521, -0.0684, -0.3120,  0.0475,\n",
       "         -0.0205,  0.1414]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequentialEx(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in net.parameters():\n",
    "    print(param)\n",
    "# Notice there are no parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implement a module that takes two modules as an argument, say net1 and net2 and returns the concatenated output of both networks in the forward propagation. This is also called a parallel module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelModule(nn.Module):\n",
    "    def __init__(self, net1, net2):\n",
    "        super().__init__()\n",
    "        self.net1 = net1\n",
    "        self.net2 = net2\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return nn.cat(self.net1(X), self.net2(X))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Assume that you want to concatenate multiple instances of the same network. Implement a factory function that generates multiple instances of the same module and build a larger network from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Factory(nn.Module):\n",
    "    def __init__(self, module, k):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential()\n",
    "        for i in range(k):\n",
    "            nn.Sequential.add_module(str(i), module)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Module.add_module() missing 1 required positional argument: 'module'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Davay\\Desktop\\D2L\\pytorch\\code\\6.1 Layers and Modules.ipynb Cell 21\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Davay/Desktop/D2L/pytorch/code/6.1%20Layers%20and%20Modules.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m net \u001b[39m=\u001b[39m Factory(nn\u001b[39m.\u001b[39;49mLazyLinear(\u001b[39m10\u001b[39;49m),\u001b[39m2\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Davay\\Desktop\\D2L\\pytorch\\code\\6.1 Layers and Modules.ipynb Cell 21\u001b[0m in \u001b[0;36mFactory.__init__\u001b[1;34m(self, module, k)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Davay/Desktop/D2L/pytorch/code/6.1%20Layers%20and%20Modules.ipynb#X36sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Davay/Desktop/D2L/pytorch/code/6.1%20Layers%20and%20Modules.ipynb#X36sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(k):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Davay/Desktop/D2L/pytorch/code/6.1%20Layers%20and%20Modules.ipynb#X36sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     nn\u001b[39m.\u001b[39;49mSequential\u001b[39m.\u001b[39;49madd_module(\u001b[39mstr\u001b[39;49m(i), module)\n",
      "\u001b[1;31mTypeError\u001b[0m: Module.add_module() missing 1 required positional argument: 'module'"
     ]
    }
   ],
   "source": [
    "net = Factory(nn.LazyLinear(10),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
