{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing pipeline for text:\n",
    "- Load text as strings into memory.\n",
    "\n",
    "- Split the strings into tokens (e.g., words or characters).\n",
    "\n",
    "- Build a vocabulary dictionary to associate each vocabulary element with a numerical index.\n",
    "\n",
    "- Convert the text into sequences of numerical indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will work with H. G. Wells’ The Time Machine, a book containing just over 30000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Time Machine, by H. G. Wells [1898]\\n\\n\\n\\n\\nI\\n\\n\\nThe Time Tra'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TimeMachine(d2l.DataModule): #@save\n",
    "    \"\"\"The Time Machine dataset.\"\"\"\n",
    "    def _download(self):\n",
    "        fname = d2l.download(d2l.DATA_URL + 'timemachine.txt', self.root,\n",
    "                             '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "        with open(fname) as f:\n",
    "            return f.read()\n",
    "data = TimeMachine()\n",
    "raw_text = data._download()\n",
    "raw_text[:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we ignore punctuation and capitalization when preprocessing the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the time machine by h g wells i the time traveller for so it'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@d2l.add_to_class(TimeMachine)  #@save\n",
    "def _preprocess(self, text):\n",
    "    return re.sub('[^A-Za-z]+', ' ', text).lower()\n",
    "\n",
    "text = data._preprocess(raw_text)\n",
    "text[:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t,h,e, ,t,i,m,e, ,m,a,c,h,i,n,e, ,b,y, ,h, ,g, ,w,e,l,l,s, '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@d2l.add_to_class(TimeMachine)  #@save\n",
    "def _tokenize(self, text):\n",
    "    return list(text)\n",
    "\n",
    "tokens = data._tokenize(text)\n",
    "','.join(tokens[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tokens are still strings. However, the inputs to our models must ultimately consist of numerical inputs. Next, we introduce a class for constructing vocabularies, i.e., objects that associate each distinct token value with a unique index. First, we determine the set of unique tokens in our training corpus. We then assign a numerical index to each unique token. Rare vocabulary elements are often dropped for convenience. Whenever we encounter a token at training or test time that had not been previously seen or was dropped from the vocabulary, we represent it by a special “<unk\\>” token, signifying that this is an unknown value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:  #@save\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        # Flatten a 2D list if needed\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        # counter.items() returns a list of tuples (token, frequency).\n",
    "        # We then base our sorting on the frequency of each element\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The list of unique tokens\n",
    "        # set() prevents duplicates from appearing and sorted allows \n",
    "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + \n",
    "                                            [token for token, freq in self.token_freqs if freq >= min_freq])))\n",
    "        self.token_to_idx = {token : idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of unique tokens\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    # __getitem is used to evaluate the value of self[key] by the object or instance of the class\n",
    "    def __getitem__(self, tokens):\n",
    "        # Check if tokens is a single token str or a list/tuple of tokens str\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            # returns idx of token if found, otherwise get '<unk>' token idx\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        # Recursive call to itself until all tokens have been converted to their associated indices\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    # Takes a list of token indices and returns their associated str representations\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return self.token_to_idx['<unk>']\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices: [21, 9, 6, 0, 21, 10, 14, 6, 0, 14]\n",
      "words: ['t', 'h', 'e', ' ', 't', 'i', 'm', 'e', ' ', 'm']\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(tokens)\n",
    "indices = vocab[tokens[:10]]\n",
    "print('indices:', indices)\n",
    "print('tokens:', vocab.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vocab = Vocab(tokens)\n",
    "indices = vocab[tokens[:10]]\n",
    "print('indices:', indices)\n",
    "print('words:', vocab.to_tokens(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
