{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(d2l.Module):  #@save\n",
    "    \"\"\"The RNN model implemented with high-level APIS\"\"\"\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.rnn = nn.RNN(num_inputs, num_hiddens)\n",
    "\n",
    "    def forward(self, inputs, H=None):\n",
    "        return self.rnn(inputs, H)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(d2l.RNNLMScratch):\n",
    "    \"\"\"The RNN-based language model implemented with high-level APIs.\"\"\"\n",
    "    def init_params(self):\n",
    "        self.linear = nn.LazyLinear(self.vocab_size)\n",
    "    \n",
    "    def output_layer(self, hiddens):\n",
    "        return self.linear(hiddens).swapaxes(0, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, let's make a prediction with a model initialized with random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it hasjujujujujujujujujuju'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n",
    "rnn = RNN(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = RNNLM(rnn, vocab_size=len(data.vocab), lr=1)\n",
    "model.predict('it has', 20, data.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Davay\\Desktop\\D2L\\pytorch\\9. Recurrent Neural Networks\\9.6 Concise RNN.ipynb Cell 9\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Davay/Desktop/D2L/pytorch/9.%20Recurrent%20Neural%20Networks/9.6%20Concise%20RNN.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m d2l\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, gradient_clip_val\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, num_gpus\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Davay/Desktop/D2L/pytorch/9.%20Recurrent%20Neural%20Networks/9.6%20Concise%20RNN.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, data)\n",
      "File \u001b[1;32mc:\\Users\\Davay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\d2l\\torch.py:268\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, data)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, model, data):\n\u001b[0;32m    267\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_data(data)\n\u001b[1;32m--> 268\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_model(model)\n\u001b[0;32m    269\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfigure_optimizers()\n\u001b[0;32m    270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Davay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\d2l\\torch.py:321\u001b[0m, in \u001b[0;36mTrainer.prepare_model\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    319\u001b[0m model\u001b[39m.\u001b[39mboard\u001b[39m.\u001b[39mxlim \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_epochs]\n\u001b[0;32m    320\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgpus:\n\u001b[1;32m--> 321\u001b[0m     model\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpus[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m    322\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n",
      "File \u001b[1;32mc:\\Users\\Davay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\Davay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Davay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Davay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:202\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    197\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_apply(fn)\n\u001b[0;32m    199\u001b[0m \u001b[39m# Resets _flat_weights\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[39m# Note: be v. careful before removing this, as 3rd party device types\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_flat_weights()\n\u001b[0;32m    204\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\Davay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:139\u001b[0m, in \u001b[0;36mRNNBase._init_flat_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights \u001b[39m=\u001b[39m [\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, wn) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, wn) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    136\u001b[0m                       \u001b[39mfor\u001b[39;00m wn \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights_names]\n\u001b[0;32m    137\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weight_refs \u001b[39m=\u001b[39m [weakref\u001b[39m.\u001b[39mref(w) \u001b[39mif\u001b[39;00m w \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    138\u001b[0m                           \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights]\n\u001b[1;32m--> 139\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflatten_parameters()\n",
      "File \u001b[1;32mc:\\Users\\Davay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:190\u001b[0m, in \u001b[0;36mRNNBase.flatten_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj_size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    189\u001b[0m     num_weights \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 190\u001b[0m torch\u001b[39m.\u001b[39;49m_cudnn_rnn_flatten_weight(\n\u001b[0;32m    191\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, num_weights,\n\u001b[0;32m    192\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_size, rnn\u001b[39m.\u001b[39;49mget_cudnn_mode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode),\n\u001b[0;32m    193\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproj_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    194\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first, \u001b[39mbool\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
